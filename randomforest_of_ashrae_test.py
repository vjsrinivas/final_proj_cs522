# -*- coding: utf-8 -*-
"""Copy of ASHRAE_Test

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ijjD4AaJ2PoW2EaGf9N6z6vDX7UitcrJ

# Reduce Memory Footprint
"""

import gdown
ashrae_url = "https://drive.google.com/uc?id=1UWf5PYpIPv5TJnOE3eVZOekBiryBWobu"
output = "ashrae.zip"
gdown.download(ashrae_url, output, quiet=False)

# Unzip ASHRAE data:
!unzip ashrae.zip
!rm -rf ashrae.zip

# Load in training data:
import pandas as pd
import numpy as np

train = pd.read_csv('train.csv')
# Find the current data footprint of train dataframe:
train.info()
print()

# Reduce datatype precision to lower memory:
#print(np.max(train["building_id"])) # maximum number is 1448 so we can fit it in int32
#print(np.max(train["meter"])) # to uint8 (range from 1 to 3 lol)
#print(np.max(train["meter_reading"])) # leave as float64
train["building_id"] = train["building_id"].astype(np.int32)
train["meter"] = train["meter"].astype(np.uint8)

# new memory footprint:
train.info()

# Load in testing data:
test = pd.read_csv('test.csv')
# Find the current data footprint of train dataframe:
test.info()
print()

# Reduce datatype precision to lower memory:
#print(np.max(test["row_id"])) # maximum number is 41 mil so we can fit it in int32
#print(np.max(test["building_id"])) # max number is 1448 so int32
#print(np.max(test["meter"])) # to uint8 (range from 1 to 3 lol)
test["row_id"] = test["row_id"].astype(np.int32)
test["building_id"] = test["building_id"].astype(np.int32)
test["meter"] = test["meter"].astype(np.uint8)

# new memory footprint:
test.info() # reduced 1.2 GB to 676 MB

weather_train = pd.read_csv('weather_train.csv')
# Find the current data footprint of train dataframe:
weather_train.info()
# no point in reducing 9.6 MB memory footprint...
print()
556
weather_test = pd.read_csv('weather_test.csv')
weather_test.info() # 19 MB memory
print()

meta = pd.read_csv('building_metadata.csv')
meta.info() # KB...

"""# Visualization"""

# TODO...
train

weather_train

"""# Load Pre-Processed Data (Cleaned Up Data)"""

import gdown
import pandas as pd
import numpy as np

ashrae_url = "https://drive.google.com/uc?id=1UlIpiR3Y6XiSWLhJGuaogrFgYsJns2FN"
output = "cleaned_ashrae.csv"
gdown.download(ashrae_url, output, quiet=False)

clean_train = pd.read_csv('cleaned_ashrae.csv') # with ALL 14 features

# reduce memory:
clean_train['timestamp'] = clean_train['timestamp'].astype(np.int64)
clean_train['meter'] = clean_train['meter'].astype(np.uint8)
clean_train['site_id'] = clean_train['site_id'].astype(np.int64)
clean_train['primary_use'] = clean_train['primary_use'].astype(np.int64)

clean_train = clean_train.drop(columns='floor_count')
clean_train = clean_train.drop(columns='year_built')
clean_train # NOTE: meter_reading is the target variable so is the 14th feature

clean_train.info()

#Taking random 10% sample of the data as test sample
sample_train=clean_train.sample(frac=0.10)

#The new shape of the data
sample_train.shape

#Spliting into features and target, so we remove the target
X = sample_train.drop('meter_reading', axis = 1)
Y = sample_train['meter_reading']

X.shape

Y.shape

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

#Standardizing the features we using standard scalar
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 20, random_state =30)
model.fit(X_train_scaled,Y_train)

Y_predict = model.predict(X_test_scaled)

X_test_scaled.shape

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

mse = mean_squared_error(Y_test,Y_predict)
mbe = mean_absolute_error(Y_test,Y_predict)

mse

mbe

def rmsle_metric(Y_test,Y_predict):
  #Convert the pandas series to array
  Y_test_array = Y_test.to_numpy()
  n = len(Y_test_array)
  value_list = []
  for i in range(n):
    pri = Y_predict[i]
    act = Y_test_array[i]
    log_pred = np.log(pri + 1)
    log_act = np.log(act + 1)
    value = (log_pred - log_act)**2
    value_list.append(value)

  msle = sum(value_list)/n
  rmsle = np.sqrt(msle)
  return rmsle

rmsle_value = rmsle_metric(Y_test,Y_predict)
rmsle_value